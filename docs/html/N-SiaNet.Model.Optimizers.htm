<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"[]>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="save" content="history" />
    <title>SiaNet.Model.Optimizers Namespace</title>
    <link rel="stylesheet" type="text/css" href="../styles/lightweight.css" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" />
    <link rel="stylesheet" type="text/css" href="../styles/lw-code.css" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" />
    <script type="text/javascript" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" src="../scripts/languageSelector.js"> </script>
    <meta name="container" content="SiaNet.Model.Optimizers" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" />
    <meta name="file" content="N-SiaNet.Model.Optimizers" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" />
    <meta name="guid" content="N-SiaNet.Model.Optimizers" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt" />
    <xml xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt">
      <MSHelp:Attr Name="AssetID" Value="N:SiaNet.Model.Optimizers" />
      <MSHelp:TOCTitle Title="SiaNet.Model.Optimizers Namespace" />
      <MSHelp:RLTitle Title="SiaNet.Model.Optimizers Namespace ()" />
      <MSHelp:Keyword Index="A" Term="N:SiaNet.Model.Optimizers" />
      <MSHelp:Keyword Index="A" Term="frlrfSiaNetModelOptimizers" />
      <MSHelp:Keyword Index="K" Term="SiaNet.Model.Optimizers namespace" />
      <MSHelp:Keyword Index="F" Term="SiaNet.Model.Optimizers" />
      <MSHelp:Attr Name="HelpPriority" Value="1" />
      <MSHelp:Attr Name="DevLang" Value="CSharp" />
      <MSHelp:Attr Name="DevLang" Value="VB" />
      <MSHelp:Attr Name="Locale" Value="en-us" />
      <MSHelp:Attr Name="TopicType" Value="kbSyntax" />
      <MSHelp:Attr Name="TopicType" Value="apiref" />
    </xml>
  </head>
  <body>
    <div class="contentPlaceHolder" xmlns:MSHelp="http://msdn.microsoft.com/mshelp" xmlns:mshelp="http://msdn.microsoft.com/mshelp" xmlns:ddue="http://ddue.schemas.microsoft.com/authoring/2003/5" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:msxsl="urn:schemas-microsoft-com:xslt">
      <div class="content">
        <div class="topicContainer">
          <div class="topic">
            <p class="majorTitle" />
            <h1 class="title">SiaNet.Model.Optimizers Namespace</h1>
            <div id="mainSection">
              <div id="mainBody">
                <div class="LW_CollapsibleArea_Container">
                  <div class="LW_CollapsibleArea_TitleDiv">
                    <h2 class="LW_CollapsibleArea_Title">Classes</h2>
                    <div class="LW_CollapsibleArea_HrDiv">
                      <hr class="LW_CollapsibleArea_Hr" />
                    </div>
                  </div>
                  <a id="classSection">
                    <!---->
                  </a>
                  <table id="typeList" class="members">
                    <tr>
                      <th class="iconColumn">
           
       </th>
                      <th class="nameColumn">Class</th>
                      <th class="descriptionColumn">Description</th>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.AdaDelta.htm">AdaDelta</a>
                      </td>
                      <td>
                        <div class="summary">
   Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.AdaGrad.htm">AdaGrad</a>
                      </td>
                      <td>
                        <div class="summary">
   Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.Adam.htm">Adam</a>
                      </td>
                      <td>
                        <div class="summary">
   Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vtvt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mtmt, similar to momentum.
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.Adamax.htm">Adamax</a>
                      </td>
                      <td>
                        <div class="summary">
   The Vt factor in the Adam update rule scales the gradient inversely proportionally to the ℓ2 norm of the past gradients (via the vt−1 term) and current gradient.
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.BaseOptimizer.htm">BaseOptimizer</a>
                      </td>
                      <td />
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.MomentumSGD.htm">MomentumSGD</a>
                      </td>
                      <td>
                        <div class="summary">
   Momentum of Stochastic gradient descent optimizer.
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.RMSProp.htm">RMSProp</a>
                      </td>
                      <td>
                        <div class="summary">
   RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton. This optimizer is usually a good choice for recurrent neural networks.
   </div>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <img src="../icons/pubclass.gif" alt="Public class" title="Public class" />
                      </td>
                      <td>
                        <a href="T-SiaNet.Model.Optimizers.SGD.htm">SGD</a>
                      </td>
                      <td>
                        <div class="summary">
   SGD is an optimisation technique. It is an alternative to Standard Gradient Descent and other approaches like batch training or BFGS. It still leads to fast convergence, with some advantages:
   - Doesn't require storing all training data in memory (good for large training sets)
   - Allows adding new data in an "online" setting
   </div>
                      </td>
                    </tr>
                  </table>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div id="footer" class="footerContainer">
          <div class="footerLogoContainer">
            <div style="margin-top:5px">
              <span class="customCopyrightFooter" />
            </div>
            <div style="margin-top:5px" class="copyrightFooter">Help File generated with GhostDoc</div>
            <div> </div>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>